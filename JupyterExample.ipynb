{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb51970-1b9d-40e5-bf25-221bbfaef839",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2869e7f-8ee3-49a3-b587-8ba68dfd92e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import time\n",
    "from dask import delayed\n",
    "from dask import delayed, persist\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fbd3cb-7545-4884-b38b-ff236538c1af",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6d554-a0ef-4aa3-bab1-f18ffca9c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '1979-01-06'\n",
    "end_date = '1979-03-05' #make sure this date is after the start date... \n",
    "interval_hours = 1 #what hour interval would you like to get? [i.e: 1 = 24 files/day, 6 = 4 files/day]\n",
    "FPout = '/glade/scratch/wchapman/ERA5_regrid_out/' #where do you want the files stored?\n",
    "prefix_out = 'ERA5_e5.oper.ml.v3' #what prefix do you want the files stored with?\n",
    "use_multithreading = False  # Set to True for multi-threading or False for multi-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b6ef090a-3948-4aec-b9ec-16b70ebb9a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...shutdown client...\n"
     ]
    }
   ],
   "source": [
    "if 'client' in locals():\n",
    "    client.shutdown()\n",
    "    print('...shutdown client...')\n",
    "else:\n",
    "    print('client does not exist yet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247efdd6-b652-4f58-a6ff-20d74762a8c6",
   "metadata": {},
   "source": [
    "## Get the Distributed Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4c5be945-3d7b-42d1-a6d6-5fbed0e4b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "from ncar_jobqueue import NCARCluster\n",
    "\n",
    "cluster = NCARCluster(project='P54048000',walltime='11:00:00')\n",
    "cluster.scale(40)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071cadd-af4a-49f8-9212-23f7bbb2124c",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83af01c9-5c01-42fc-8193-e17c3690ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert that dates wanted > 0\n",
    "Dateswanted = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'H')\n",
    "Dayswanted = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'D')\n",
    "\n",
    "Static_zheight = xr.open_dataset('/glade/u/home/wchapman/RegriddERA5_CAMFV/static_operation_ERA5_zhght.nc')\n",
    "\n",
    "def find_strings_with_substring(string_list, substring):\n",
    "    # Initialize an empty list to store matching strings\n",
    "    matching_strings = []\n",
    "\n",
    "    # Iterate through the list\n",
    "    for string in string_list:\n",
    "        # Check if the specified substring is present in the current string\n",
    "        if substring in string:\n",
    "            matching_strings.append(string)\n",
    "\n",
    "    # Return the list of matching strings\n",
    "    return matching_strings\n",
    "\n",
    "def flatten_list(input_list):\n",
    "    flattened_list = []\n",
    "    for item in input_list:\n",
    "        if isinstance(item, list):\n",
    "            flattened_list.extend(flatten_list(item))\n",
    "        else:\n",
    "            flattened_list.append(item)\n",
    "    return flattened_list\n",
    "\n",
    "##function get file paths ... \n",
    "def fp_dates_wanted(Dateswanted):\n",
    "    years_wanted = Dateswanted[:].year\n",
    "    months_wanted = Dateswanted[:].month\n",
    "    day_wanted = Dateswanted[:].day\n",
    "    \n",
    "    list_yrm =[]\n",
    "    for ywmw in zip(years_wanted,months_wanted):\n",
    "        list_yrm.append(str(ywmw[0])+f'{ywmw[1]:02}')\n",
    "    \n",
    "    fp_t = []\n",
    "    fp_u = []\n",
    "    fp_v = []\n",
    "    fp_q = []\n",
    "    fp_ps = []\n",
    "    \n",
    "    lastday = str(Dateswanted[-1])[:10]\n",
    "    \n",
    "    for yrm_fp in np.unique(list_yrm):\n",
    "        for dayday in np.unique(day_wanted):\n",
    "            \n",
    "            \n",
    "            fp_u.append(sorted(glob.glob('/glade/collections/rda/data/ds633.6/e5.oper.an.ml/'+yrm_fp+'/'+'*_u*'+yrm_fp+f'{dayday:02}'+'*.nc')))\n",
    "            fp_v.append(sorted(glob.glob('/glade/collections/rda/data/ds633.6/e5.oper.an.ml/'+yrm_fp+'/'+'*_v*'+yrm_fp+f'{dayday:02}'+'*.nc')))\n",
    "            fp_t.append(sorted(glob.glob('/glade/collections/rda/data/ds633.6/e5.oper.an.ml/'+yrm_fp+'/'+'*_t*'+yrm_fp+f'{dayday:02}'+'*.nc')))\n",
    "            fp_q.append(sorted(glob.glob('/glade/collections/rda/data/ds633.6/e5.oper.an.ml/'+yrm_fp+'/'+'*_q*'+yrm_fp+f'{dayday:02}'+'*.nc')))\n",
    "            fp_ps.append(sorted(glob.glob('/glade/collections/rda/data/ds633.6/e5.oper.an.ml/'+yrm_fp+'/'+'*_sp*'+yrm_fp+f'{dayday:02}'+'*.nc')))\n",
    "            \n",
    "            if yrm_fp[:4]+'-'+yrm_fp[4:]+'-'+f'{dayday:02}' == lastday:\n",
    "                break\n",
    "\n",
    "    fp_u = flatten_list(fp_u)\n",
    "    fp_v = flatten_list(fp_v)\n",
    "    fp_t = flatten_list(fp_t)\n",
    "    fp_q = flatten_list(fp_q)\n",
    "    fp_ps = flatten_list(fp_ps)\n",
    "    \n",
    "    files_dict ={'u':np.unique(fp_u),'v':np.unique(fp_v),'t':np.unique(fp_t),'q':np.unique(fp_q),'ps':np.unique(fp_ps)}\n",
    "    \n",
    "    \n",
    "    return files_dict \n",
    "\n",
    "files_dict=fp_dates_wanted(Dateswanted)\n",
    "\n",
    "def make_nc_files(files_dict,Dateswanted,Dayswanted):    \n",
    "    for dw in Dayswanted:\n",
    "        print(str(dw)[:10])\n",
    "        substring_match = str(dw)[:4]+str(dw)[5:7]+str(dw)[8:10]\n",
    "        smatch_u = find_strings_with_substring(files_dict['u'], substring_match)\n",
    "        smatch_v = find_strings_with_substring(files_dict['v'], substring_match)\n",
    "        smatch_t = find_strings_with_substring(files_dict['t'], substring_match)\n",
    "        smatch_q = find_strings_with_substring(files_dict['q'], substring_match)\n",
    "        smatch_ps = find_strings_with_substring(files_dict['ps'], substring_match)\n",
    "        DS_u= xr.open_mfdataset(smatch_u)\n",
    "        sel_times = Dateswanted.intersection(DS_u['time'])\n",
    "        DS_v= xr.open_mfdataset(smatch_v).sel(time=sel_times)\n",
    "        DS_t= xr.open_mfdataset(smatch_t).sel(time=sel_times)\n",
    "        DS_q= xr.open_mfdataset(smatch_q).sel(time=sel_times)\n",
    "        DS_ps= xr.open_mfdataset(smatch_ps).sel(time=sel_times)\n",
    "        print('loading')\n",
    "        DS=xr.merge([DS_u.sel(time=sel_times),DS_v,DS_t,DS_q]).load()\n",
    "        print('loaded')\n",
    "        \n",
    "        for ee,tt in enumerate(DS['time']):\n",
    "            hourdo = DS['time.hour'][ee]\n",
    "            \n",
    "            datstr = str(dw)[:4]+str(dw)[5:7]+str(dw)[8:10]+f'{hourdo:02}'\n",
    "            #DS.sel(time=tt).squeeze().to_netcdf()\n",
    "            out_file=+'/' +prefix_out +'.uvtq.'+ datstr+'.nc'\n",
    "            write_job = DS.sel(time=tt).squeeze().to_netcdf(out_file,compute=False)\n",
    "            with ProgressBar():\n",
    "                print(f\"Writing to {out_file}\")\n",
    "                write_job.compute()      \n",
    "            print(out_file) \n",
    "            out_file=FPout+'/' +prefix_out +'.ps.'+ datstr+'.nc'\n",
    "            DS_ps['Z_GDS4_SFC'] = xr.zeros_like(DS_ps['SP'])\n",
    "            DS_ps['Z_GDS4_SFC'][:,:]=Static_zheight['Z_GDS4_SFC'].values\n",
    "            write_job = DS_ps.sel(time=tt).squeeze().to_netcdf(out_file,compute=False)\n",
    "            with ProgressBar():\n",
    "                print(f\"Writing to {out_file}\")\n",
    "                write_job.compute()    \n",
    "            print(out_file) \n",
    "\n",
    "    return DS,DS_ps\n",
    "\n",
    "\n",
    "def add_staggered_grid(FPout,prefix_out):\n",
    "    \n",
    "    prefix_out = 'test_out_'\n",
    "    all_files = sorted(glob.glob(FPout+'/'+prefix_out+'??????????.nc'))\n",
    "    \n",
    "    for fdfd in all_files:\n",
    "        print(fdfd)\n",
    "        BB = xr.open_dataset(fdfd)\n",
    "        bbus = xr.zeros_like(BB['U']).to_dataset(name='US')\n",
    "        bbus['US'][:,:]=BB['U']\n",
    "        bbvs = xr.zeros_like(BB['V']).to_dataset(name='VS')\n",
    "        bbvs['VS'][:,:]=BB['V']\n",
    "        bball = xr.merge([BB,bbus,bbvs]).chunk()\n",
    "        bball.to_netcdf(fdfd[:-13]+'.s.'+fdfd[-13:])   \n",
    "        os.remove(fdfd)\n",
    "    return all_files,BB\n",
    "\n",
    "\n",
    "\n",
    "import xarray as xr\n",
    "from dask import delayed\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def make_nc_files_optimized(files_dict, Dateswanted, Dayswanted, FPout, prefix_out):\n",
    "    \"\"\"\n",
    "    Optimized function to perform a specific task using Dask with specified resources.\n",
    "\n",
    "    Parameters:\n",
    "    - files_dict: A dictionary of files.\n",
    "    - Dateswanted: List of dates.\n",
    "    - Dayswanted: List of days.\n",
    "    - FPout: Output file path.\n",
    "    - prefix_out: Output file prefix.\n",
    "\n",
    "    Returns:\n",
    "    - delayed_writes: List of delayed write operations.\n",
    "    \"\"\"\n",
    "    Static_zheight = xr.open_dataset('/glade/u/home/wchapman/RegriddERA5_CAMFV/static_operation_ERA5_zhght.nc')\n",
    "    \n",
    "    delayed_writes = []\n",
    "    for dw in Dayswanted:\n",
    "        print(str(dw)[:10])\n",
    "        substring_match = str(dw)[:4] + str(dw)[5:7] + str(dw)[8:10]\n",
    "        smatch_u = find_strings_with_substring(files_dict['u'], substring_match)\n",
    "        smatch_v = find_strings_with_substring(files_dict['v'], substring_match)\n",
    "        smatch_t = find_strings_with_substring(files_dict['t'], substring_match)\n",
    "        smatch_q = find_strings_with_substring(files_dict['q'], substring_match)\n",
    "        smatch_ps = find_strings_with_substring(files_dict['ps'], substring_match)\n",
    "        \n",
    "        DS_u = xr.open_mfdataset(smatch_u, parallel=True)\n",
    "        sel_times = Dateswanted.intersection(DS_u['time'])\n",
    "        DS_v = xr.open_mfdataset(smatch_v, parallel=True).sel(time=sel_times)\n",
    "        DS_t = xr.open_mfdataset(smatch_t, parallel=True).sel(time=sel_times)\n",
    "        DS_q = xr.open_mfdataset(smatch_q, parallel=True).sel(time=sel_times)\n",
    "        DS_ps = xr.open_mfdataset(smatch_ps, parallel=True).sel(time=sel_times)\n",
    "        \n",
    "        print('loading')\n",
    "        DS = xr.merge([DS_u.sel(time=sel_times), DS_v, DS_t, DS_q])\n",
    "        print('copying variables')\n",
    "        DS['US'] = DS['U'].copy(deep=True)\n",
    "        DS['VS'] = DS['V'].copy(deep=True)\n",
    "        print('loaded')\n",
    "        \n",
    "        for ee, tt in enumerate(DS['time']):\n",
    "            hourdo = DS['time.hour'][ee]\n",
    "            datstr = str(dw)[:4] + str(dw)[5:7] + str(dw)[8:10] + f'{hourdo:02}'\n",
    "            \n",
    "            out_file_uvtq = FPout + '/' + prefix_out + '.uvtq.' + datstr + '.nc'\n",
    "            delayed_write_uvtq = delayed(DS.sel(time=tt).squeeze().to_netcdf)(out_file_uvtq)\n",
    "            delayed_writes.append(delayed_write_uvtq)\n",
    "            \n",
    "            out_file_ps = FPout + '/' + prefix_out + '.ps.' + datstr + '.nc'\n",
    "            DS_ps['Z_GDS4_SFC'] = xr.zeros_like(DS_ps['SP'])\n",
    "            DS_ps['Z_GDS4_SFC'][:, :] = Static_zheight['Z_GDS4_SFC'].values\n",
    "            delayed_write_ps = delayed(DS_ps.sel(time=tt).squeeze().to_netcdf)(out_file_ps)\n",
    "            delayed_writes.append(delayed_write_ps)\n",
    "\n",
    "    # Compute the delayed write operations concurrently\n",
    "    with ProgressBar():\n",
    "        delayed_writes = list(dask.compute(*delayed_writes))\n",
    "\n",
    "    return delayed_writes\n",
    "\n",
    "\n",
    "\n",
    "def divide_datetime_index(date_index, max_items_per_division=4):\n",
    "    \"\"\"\n",
    "    Divide a DatetimeIndex into sublists with a maximum number of items per division.\n",
    "\n",
    "    Parameters:\n",
    "    - date_index: DatetimeIndex to be divided.\n",
    "    - max_items_per_division: Maximum number of items per division (default is 4).\n",
    "\n",
    "    Returns:\n",
    "    - divided_lists: List of sublists.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the divided lists\n",
    "    divided_lists = []\n",
    "\n",
    "    # Initialize a sublist with the first date\n",
    "    sublist = [date_index[0]]\n",
    "\n",
    "    # Iterate through the remaining dates\n",
    "    for date in date_index[1:]:\n",
    "        # Add the current date to the sublist\n",
    "        sublist.append(date)\n",
    "\n",
    "        # Check if the sublist has reached the maximum allowed size\n",
    "        if len(sublist) == max_items_per_division:\n",
    "            # If it has, add the sublist to the divided_lists and reset the sublist\n",
    "            divided_lists.append(sublist)\n",
    "            sublist = []\n",
    "\n",
    "    # If there are remaining items in the sublist, add it to the divided_lists\n",
    "    if sublist:\n",
    "        divided_lists.append(sublist)\n",
    "\n",
    "    # Ensure that every division has at least two items by merging the last two divisions if necessary\n",
    "    if len(divided_lists[-1]) < 2 and len(divided_lists) > 1:\n",
    "        last_two_lists = divided_lists[-2:]  # Get the last two divisions\n",
    "        combined_list = sum(last_two_lists, [])  # Combine them\n",
    "        divided_lists = divided_lists[:-2]  # Remove the last two divisions\n",
    "        divided_lists.append(combined_list)  # Add the combined list back\n",
    "\n",
    "    return divided_lists\n",
    "\n",
    "def increment_date_by_one_day(date_str):\n",
    "    \"\"\"\n",
    "    Increment a date by one day and return it as a string.\n",
    "\n",
    "    Parameters:\n",
    "    - date_str: Input date string in the format 'YYYY-MM-DD'.\n",
    "\n",
    "    Returns:\n",
    "    - incremented_date_str: Date string incremented by one day.\n",
    "    \"\"\"\n",
    "    # Convert the input date string to a pandas Timestamp\n",
    "    date = pd.Timestamp(date_str)\n",
    "\n",
    "    # Increment the date by one day\n",
    "    incremented_date = date + pd.DateOffset(days=1)\n",
    "\n",
    "    # Convert the incremented date back to a string in the same format\n",
    "    incremented_date_str = incremented_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    return incremented_date_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b114590-2d59-40d6-858a-85a9a4235c19",
   "metadata": {},
   "source": [
    "## Run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6b1bce5d-f49b-4a01-ab16-a3d8ff4769f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at all the dates:\n",
    "Dayswantedtot = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'D')\n",
    "#look at all the dates:\n",
    "\n",
    "print(len(Dayswantedtot))\n",
    "\n",
    "if len(Dayswantedtot)<4:\n",
    "    start_time = time.time()  # Record the start time\n",
    "    Dayswanted = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'D')\n",
    "    Dateswanted = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'H')\n",
    "    Static_zheight = xr.open_dataset('/glade/u/home/wchapman/RegriddERA5_CAMFV/static_operation_ERA5_zhght.nc')\n",
    "    files_dict=fp_dates_wanted(Dateswanted)\n",
    "    #make the files:\n",
    "    print('...starting processing...')\n",
    "    delayed_writes = make_nc_files_optimized(files_dict, Dateswanted, Dayswanted,FPout, prefix_out)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\" executed in {elapsed_time} seconds\")\n",
    "else: \n",
    "    print('in here!!')\n",
    "    divided_lists =divide_datetime_index(Dayswantedtot)\n",
    "    \n",
    "    for dd in divided_lists:\n",
    "        strtd = str(dd[0])[:10]\n",
    "        endd  = str(dd[-1])[:10]\n",
    "        endd  = increment_date_by_one_day(endd)\n",
    "        print('doing files:',strtd,endd)\n",
    "        start_time = time.time()  # Record the start time\n",
    "        Dayswanted = pd.date_range(start=strtd,end=endd,freq=str(interval_hours)+'D')\n",
    "        Dateswanted = pd.date_range(start=strtd,end=endd,freq=str(interval_hours)+'H')\n",
    "        Static_zheight = xr.open_dataset('/glade/u/home/wchapman/RegriddERA5_CAMFV/static_operation_ERA5_zhght.nc')\n",
    "        files_dict=fp_dates_wanted(Dateswanted)\n",
    "        #make the files:\n",
    "        print('...starting processing...')\n",
    "        delayed_writes = make_nc_files_optimized(files_dict, Dateswanted, Dayswanted,FPout, prefix_out)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\" phase executed in {elapsed_time} seconds\")\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5479980-5696-4cec-a9fb-f056d7359a88",
   "metadata": {},
   "source": [
    "## Ignore Below Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578cae9b-14dd-4138-9101-88f2eb52c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a range of dates from start_date to end_date with the specified interval_hours\n",
    "Dayswantedtot = pd.date_range(start=start_date, end=end_date, freq=str(interval_hours)+'D')\n",
    "\n",
    "# Print the number of dates in the range\n",
    "print(len(Dayswantedtot))\n",
    "\n",
    "# Check if the number of dates is less than 4\n",
    "if len(Dayswantedtot) < 4:\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate lists of dates and datetime stamps with the specified interval_hours\n",
    "    Dayswanted = pd.date_range(start=start_date, end=end_date, freq=str(interval_hours)+'D')\n",
    "    Dateswanted = pd.date_range(start=start_date, end=end_date, freq=str(interval_hours)+'H')\n",
    "    \n",
    "    # Open the static z-height dataset\n",
    "    Static_zheight = xr.open_dataset('/glade/u/home/wchapman/RegriddERA5_CAMFV/static_operation_ERA5_zhght.nc')\n",
    "    \n",
    "    # Create a dictionary of files based on the desired dates\n",
    "    files_dict = fp_dates_wanted(Dateswanted)\n",
    "    \n",
    "    # Start processing and create delayed write operations\n",
    "    print('...starting processing...')\n",
    "    delayed_writes = make_nc_files_optimized(files_dict, Dateswanted, Dayswanted, FPout, prefix_out)\n",
    "    \n",
    "    # Calculate and print the elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\" executed in {elapsed_time} seconds\")\n",
    "else:\n",
    "    print('in here!!')\n",
    "    \n",
    "    # Divide the list of dates into sublists with a maximum of 4 items per sublist\n",
    "    divided_lists = divide_datetime_index(Dayswantedtot)\n",
    "    \n",
    "    # Iterate through the divided date sublists\n",
    "    for dd in divided_lists:\n",
    "        strtd = str(dd[0])[:10]\n",
    "        endd = str(dd[-1])[:10]\n",
    "        \n",
    "        # Increment the end date by one day\n",
    "        endd = increment_date_by_one_day(endd)\n",
    "        \n",
    "        print('doing files:', strtd, endd)\n",
    "        \n",
    "        # Record the start time for this phase\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate lists of dates and datetime stamps with the specified interval_hours for this phase\n",
    "        Dayswanted = pd.date_range(start=strtd, end=endd, freq=str(interval_hours)+'D')\n",
    "        Dateswanted = pd.date_range(start=strtd, end=endd, freq=str(interval_hours)+'H')\n",
    "        \n",
    "        # Open the static z-height dataset\n",
    "        Static_zheight = xr.open_dataset('/glade/u/home/wchapman/RegriddERA5_CAMFV/static_operation_ERA5_zhght.nc')\n",
    "        \n",
    "        # Create a dictionary of files based on the desired dates\n",
    "        files_dict = fp_dates_wanted(Dateswanted)\n",
    "        \n",
    "        # Start processing and create delayed write operations for this phase\n",
    "        print('...starting processing...')\n",
    "        delayed_writes = make_nc_files_optimized(files_dict, Dateswanted, Dayswanted, FPout, prefix_out)\n",
    "        \n",
    "        # Calculate and print the elapsed time for this phase\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\" phase executed in {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "050595ee-948c-4049-bad8-e32796f16921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timing!\n",
      "...search for the right days...\n",
      "...starting processing...\n",
      "1979-01-06\n",
      "loading\n",
      "copying variables\n",
      "loaded\n",
      "1979-01-07\n",
      "loading\n",
      "copying variables\n",
      "loaded\n",
      "1979-01-08\n",
      "loading\n",
      "copying variables\n",
      "loaded\n",
      "1979-01-09\n",
      "loading\n",
      "copying variables\n",
      "loaded\n",
      " executed in 378.1310420036316 seconds\n"
     ]
    }
   ],
   "source": [
    "print('timing!')\n",
    "start_time = time.time()  # Record the start time\n",
    "print(\"...search for the right days...\")\n",
    "#determine the desired days:\n",
    "Dayswantedtot = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'D')\n",
    "\n",
    "\n",
    "Dateswanted = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'H')\n",
    "Static_zheight = xr.open_dataset('/glade/u/home/wchapman/RegriddERA5_CAMFV/static_operation_ERA5_zhght.nc')\n",
    "files_dict=fp_dates_wanted(Dateswanted)\n",
    "    \n",
    "#make the files:\n",
    "print('...starting processing...')\n",
    "delayed_writes = make_nc_files_optimized(files_dict, Dateswanted, Dayswanted,FPout, prefix_out)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\" executed in {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f30aa359-9703-463b-aa78-4084a72c3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time\n",
    "files_dict=fp_dates_wanted(Dateswanted)\n",
    "# ds,dsp = make_nc_files(files_dict,Dateswanted,Dayswanted)\n",
    "# allf, bb = add_staggered_grid(FPout,prefix_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b518498-b6ab-4aeb-bda9-be49d3fd86b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.multiprocessing\n",
    "import dask.threaded\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# def make_nc_files_optimized(files_dict, Dateswanted, Dayswanted):\n",
    "#     # Set up Dask for parallelism\n",
    "#     dask.config.set(scheduler='threads')  # Use threads for parallelism\n",
    "    \n",
    "#     for dw in Dayswanted:\n",
    "#         print(str(dw)[:10])\n",
    "#         substring_match = str(dw)[:4] + str(dw)[5:7] + str(dw)[8:10]\n",
    "#         smatch_u = find_strings_with_substring(files_dict['u'], substring_match)\n",
    "#         smatch_v = find_strings_with_substring(files_dict['v'], substring_match)\n",
    "#         smatch_t = find_strings_with_substring(files_dict['t'], substring_match)\n",
    "#         smatch_q = find_strings_with_substring(files_dict['q'], substring_match)\n",
    "#         smatch_ps = find_strings_with_substring(files_dict['ps'], substring_match)\n",
    "        \n",
    "#         DS_u = xr.open_mfdataset(smatch_u, parallel=True)\n",
    "#         sel_times = Dateswanted.intersection(DS_u['time'])\n",
    "#         DS_v = xr.open_mfdataset(smatch_v, parallel=True).sel(time=sel_times)\n",
    "#         DS_t = xr.open_mfdataset(smatch_t, parallel=True).sel(time=sel_times)\n",
    "#         DS_q = xr.open_mfdataset(smatch_q, parallel=True).sel(time=sel_times)\n",
    "#         DS_ps = xr.open_mfdataset(smatch_ps, parallel=True).sel(time=sel_times)\n",
    "        \n",
    "#         print('loading')\n",
    "#         DS = xr.merge([DS_u.sel(time=sel_times), DS_v, DS_t, DS_q])\n",
    "#         print('loaded')\n",
    "#         delayed_writes = []\n",
    "\n",
    "#         for ee, tt in enumerate(DS['time']):\n",
    "#             hourdo = DS['time.hour'][ee]\n",
    "            \n",
    "#             datstr = str(dw)[:4] + str(dw)[5:7] + str(dw)[8:10] + f'{hourdo:02}'\n",
    "#             out_file = FPout + '/' + prefix_out + '.uvtq.' + datstr + '.nc'\n",
    "#             write_job = DS.sel(time=tt).squeeze().to_netcdf(out_file, compute=False)\n",
    "#             delayed_write = delayed(DS.sel(time=tt).squeeze().to_netcdf)(out_file, compute=False)\n",
    "#             delayed_writes.append(delayed_write)\n",
    "            \n",
    "#             with ProgressBar():\n",
    "#                 print(f\"Writing to {out_file}\")\n",
    "#                 write_job.compute()\n",
    "#             print(out_file)\n",
    "            \n",
    "#             out_file = FPout + '/' + prefix_out + '.ps.' + datstr + '.nc'\n",
    "#             DS_ps['Z_GDS4_SFC'] = xr.zeros_like(DS_ps['SP'])\n",
    "#             DS_ps['Z_GDS4_SFC'][:, :] = Static_zheight['Z_GDS4_SFC'].values\n",
    "#             delayed_write = delayed(DS.sel(time=tt).squeeze().to_netcdf)(out_file, compute=False)\n",
    "#             delayed_writes.append(delayed_write)\n",
    "            \n",
    "#             with ProgressBar():\n",
    "#                 print(f\"Writing to {out_file}\")\n",
    "#                 write_job.compute()\n",
    "#             print(out_file)\n",
    "\n",
    "#     return delayed_writes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7ad617bf-6cf6-4d44-a826-b7a8664a8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dict=fp_dates_wanted(Dateswanted)\n",
    "delayed_writes = make_nc_files_optimized(files_dict,Dateswanted,Dayswanted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93af57af-bba9-442d-8eb6-9ef72e289f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.multiprocessing\n",
    "import dask.threaded\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b20d76a-c742-4dfe-ad86-3c6e24df13f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dask cluster with specified resources\n",
    "def create_dask_cluster(n_workers=1, threads_per_worker=1, memory_limit=None):\n",
    "    cluster = LocalCluster(\n",
    "        n_workers=n_workers,\n",
    "        threads_per_worker=threads_per_worker,\n",
    "        memory_limit=memory_limit\n",
    "    )\n",
    "    return cluster\n",
    "\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1, memory_limit='4GB')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94c3596d-9044-43db-bd07-d64d09c555f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_dict=fp_dates_wanted(Dateswanted)\n",
    "delayed_writes = make_nc_files_optimized(files_dict,Dateswanted,Dayswanted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31d9b46-6460-4340-86a6-8059d8da98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ee, tt in enumerate(DS['time']):\n",
    "    # ... (existing code)\n",
    "    out_file = FPout + '/' + prefix_out + '.uvtq.' + datstr + '.nc'\n",
    "    delayed_write = delayed(DS.sel(time=tt).squeeze().to_netcdf)(out_file, compute=False)\n",
    "    delayed_writes.append(delayed_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59285585-716a-43a9-87cd-200efb570444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c5f48590-9e74-44cc-8f78-9c8bc1a5ad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.threaded\n",
    "import dask.multiprocessing\n",
    "\n",
    "def make_nc_files(files_dict, Dateswanted, Dayswanted):\n",
    "    print('starting')\n",
    "    def process_day(dw):\n",
    "        print(str(dw)[:10])\n",
    "        substring_match = str(dw)[:4] + str(dw)[5:7] + str(dw)[8:10]\n",
    "        smatch_u = find_strings_with_substring(files_dict['u'], substring_match)\n",
    "        smatch_v = find_strings_with_substring(files_dict['v'], substring_match)\n",
    "        smatch_t = find_strings_with_substring(files_dict['t'], substring_match)\n",
    "        smatch_q = find_strings_with_substring(files_dict['q'], substring_match)\n",
    "        smatch_ps = find_strings_with_substring(files_dict['ps'], substring_match)\n",
    "        DS_u = xr.open_mfdataset(smatch_u)\n",
    "        sel_times = Dateswanted.intersection(DS_u['time'])\n",
    "        DS_v = xr.open_mfdataset(smatch_v).sel(time=sel_times)\n",
    "        DS_t = xr.open_mfdataset(smatch_t).sel(time=sel_times)\n",
    "        DS_q = xr.open_mfdataset(smatch_q).sel(time=sel_times)\n",
    "        DS_ps = xr.open_mfdataset(smatch_ps).sel(time=sel_times)\n",
    "        print('loading')\n",
    "        DS = xr.merge([DS_u.sel(time=sel_times), DS_v, DS_t, DS_q]).load()\n",
    "        print('loaded')\n",
    "\n",
    "        for ee, tt in enumerate(DS['time']):\n",
    "            hourdo = DS['time.hour'][ee]\n",
    "            datstr = str(dw)[:4] + str(dw)[5:7] + str(dw)[8:10] + f'{hourdo:02}'\n",
    "            out_file = FPout+'/' + prefix_out + '.uvtq.' + datstr + '.nc'\n",
    "            write_job = DS.sel(time=tt).squeeze().to_netcdf(out_file, compute=False)\n",
    "            print(f\"Queued writing to {out_file}\")\n",
    "\n",
    "            out_file = FPout + '/' + prefix_out + '.ps.' + datstr + '.nc'\n",
    "            DS_ps['Z_GDS4_SFC'] = xr.zeros_like(DS_ps['SP'])\n",
    "            DS_ps['Z_GDS4_SFC'][:, :] = Static_zheight['Z_GDS4_SFC'].values\n",
    "            write_job = DS_ps.sel(time=tt).squeeze().to_netcdf(out_file, compute=False)\n",
    "            print(f\"Queued writing to {out_file}\")\n",
    "\n",
    "        return DS, DS_ps\n",
    "\n",
    "    # Create a list of delayed tasks for each day in parallel\n",
    "    delayed_tasks = [dask.delayed(process_day)(dw) for dw in Dayswanted]\n",
    "\n",
    "    # Compute the delayed tasks in parallel using either multi-threading or multi-processing\n",
    "    if use_multithreading:\n",
    "        with dask.config.set(scheduler='threads'):\n",
    "            results = dask.compute(*delayed_tasks)\n",
    "    else:\n",
    "        with dask.config.set(scheduler='processes'):\n",
    "            results = dask.compute(*delayed_tasks)\n",
    "\n",
    "    # Collect the results\n",
    "    return results\n",
    "\n",
    "# Set use_multithreading to True or False based on your preference\n",
    "use_multithreading = True  # Set to True for multi-threading or False for multi-processing\n",
    "make_nc_files(files_dict, Dateswanted, Dayswanted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c0eea-5ad9-4cb1-b5de-90267b4252c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397805ae-fa01-482b-add8-7bb22f9312df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1015f-a09d-4968-94f6-5a769fcc5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.threaded\n",
    "import dask.multiprocessing\n",
    "\n",
    "def make_nc_files(files_dict, Dateswanted, Dayswanted, batch_size=10, use_multithreading=True):\n",
    "    def process_batch(batch):\n",
    "        batch_results = []\n",
    "        for dw in batch:\n",
    "            print(str(dw)[:10])\n",
    "            substring_match = str(dw)[:4] + str(dw)[5:7] + str(dw)[8:10]\n",
    "            smatch_u = find_strings_with_substring(files_dict['u'], substring_match)\n",
    "            smatch_v = find_strings_with_substring(files_dict['v'], substring_match)\n",
    "            smatch_t = find_strings_with_substring(files_dict['t'], substring_match)\n",
    "            smatch_q = find_strings_with_substring(files_dict['q'], substring_match)\n",
    "            smatch_ps = find_strings_with_substring(files_dict['ps'], substring_match)\n",
    "            DS_u = xr.open_mfdataset(smatch_u)\n",
    "            sel_times = Dateswanted.intersection(DS_u['time'])\n",
    "            DS_v = xr.open_mfdataset(smatch_v).sel(time=sel_times)\n",
    "            DS_t = xr.open_mfdataset(smatch_t).sel(time=sel_times)\n",
    "            DS_q = xr.open_mfdataset(smatch_q).sel(time=sel_times)\n",
    "            DS_ps = xr.open_mfdataset(smatch_ps).sel(time=sel_times)\n",
    "            print('loading')\n",
    "            DS = xr.merge([DS_u.sel(time=sel_times), DS_v, DS_t, DS_q]).load()\n",
    "            print('loaded')\n",
    "\n",
    "            for ee, tt in enumerate(DS['time']):\n",
    "                hourdo = DS['time.hour'][ee]\n",
    "                datstr = str(dw)[:4] + str(dw)[5:7] + str(dw)[8:10] + f'{hourdo:02}'\n",
    "                out_file = FPout + '/' + prefix_out + '.uvtq.' + datstr + '.nc'\n",
    "                write_job = DS.sel(time=tt).squeeze().to_netcdf(out_file, compute=False)\n",
    "                print(f\"Queued writing to {out_file}\")\n",
    "\n",
    "                out_file = FPout + '/' + prefix_out + '.ps.' + datstr + '.nc'\n",
    "                DS_ps['Z_GDS4_SFC'] = xr.zeros_like(DS_ps['SP'])\n",
    "                DS_ps['Z_GDS4_SFC'][:, :] = Static_zheight['Z_GDS4_SFC'].values\n",
    "                write_job = DS_ps.sel(time=tt).squeeze().to_netcdf(out_file, compute=False)\n",
    "                print(f\"Queued writing to {out_file}\")\n",
    "\n",
    "            batch_results.append((DS, DS_ps))\n",
    "\n",
    "        return batch_results\n",
    "\n",
    "    # Split the days into batches\n",
    "    day_batches = [Dayswanted[i:i + batch_size] for i in range(0, len(Dayswanted), batch_size)]\n",
    "\n",
    "    # Process each batch in parallel\n",
    "    all_results = []\n",
    "    for batch in day_batches:\n",
    "        batch_results = dask.delayed(process_batch)(batch)\n",
    "        with dask.config.set(scheduler='threads' if use_multithreading else 'processes'):\n",
    "            batch_results = dask.compute(batch_results)\n",
    "        all_results.extend(batch_results)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Set use_multithreading to True or False based on your preference\n",
    "use_multithreading = True  # Set to True for multi-threading or False for multi-processing\n",
    "make_nc_files(files_dict, Dateswanted, Dayswanted, use_multithreading=use_multithreading)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2023b",
   "language": "python",
   "name": "npl-2023b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
